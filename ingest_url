#!/usr/bin/env python3
from urllib.request import urlopen, Request
from argparse import ArgumentParser
import logging
import hashlib
from pathlib import Path
from time import time

logger = logging.getLogger(__name__)
logging.basicConfig(level=10)

parser = ArgumentParser()
parser.add_argument("urls", type=str, nargs='+', help="URLs to fetch")
parser.add_argument("-u,--user-agent", dest="user_agent", type=str, default="Mozilla/5.0 (X11; Linux x86_64; rv:102.0) Gecko/20100101 Firefox/102.0", help="User agent for the requests")
parser.add_argument("-o,--output", dest="output", type=Path, default=Path(__file__).parent / "data")

args = parser.parse_args()

(args.output / "http" / "sha1").mkdir(exist_ok=True, parents=True)
(args.output / "http" / "sha256").mkdir(exist_ok=True, parents=True)


logger.info("Initializing...")

def add_to_file(file: Path, item: str):
    items = set()
    if file.exists():
        for line in file.open('r'):
            line = line.strip()
            if line == "":
                continue
            items.add(line)
    items.add(item)
    items = list(items)
    items.sort()
    file.write_text('\n'.join(items))
    return items

def recon_url(url: str):
    logger.info(f"Fetching '{url}'")
    res = urlopen(
        Request(url, headers={"User-Agent": args.user_agent}),
        timeout=5
    )
    sha256hasher = hashlib.sha256()
    sha1hasher = hashlib.sha1()
    last_print=time()
    while True:
        buf = res.read(16*1024)
        if not buf:
            break
        sha1hasher.update(buf)
        sha256hasher.update(buf)
        if (time() - last_print < 5):
            logger.debug(f"Remaining for '{url}' {res.length/1024/1024}MB")
            last_print = time()
    logger.info(f"Saving hash sha1 for '{url}'")
    sha1digest = sha1hasher.hexdigest()
    sha1file = args.output / "http" / "sha1" / f"{sha1digest}.txt"
    add_to_file(sha1file, url)

    logger.info(f"Saving hash sha256 for '{url}'")
    sha256digest = sha256hasher.hexdigest()
    sha256file = args.output / "http" / "sha256" / f"{sha256digest}.txt"
    add_to_file(sha256file, url)

for url in args.urls:
    timeout_remaining = 3
    while True:
        try:
            recon_url(url)
            break
        except TimeoutError:
            if timeout_remaining <= 0:
                logger.debug("Giving up: too many retries")
                break
            logger.debug("Request timeout, retrying...")
            timeout_remaining -= 1
            continue
